---
title: "SML: Exercise 2"
author: "Thao Le, Finn-Ole HÃ¶ner, Jason Wang, Ramon Snellen"
date: "`r Sys.Date()`"
output: pdf_document2
ulrcolor: CornflowerBlue
header-includes:
  - \usepackage{float}
  - \usepackage{booktabs}
  - \usepackage{fancyvrb}
  - \usepackage{lscape}
  - \usepackage{float}
  - \newcommand{\blandscape}{\begin{landscape}}
  - \newcommand{\elandscape}{\end{landscape}}
  - \newcommand{\mat}[1]{\mathbf{#1}}
  - \newcommand{\vect}[1]{\boldsymbol{#1}}
bibliography: citations.bib
pdf-engine: xelatex
cap-location: top
toc: false
toc-title: Contents
number-sections: true
mainfont: Helvetica
setspace:
  linestretch: 1.25
fig-align: center
table-align: center
fig-pos: H
table-pos: H
execute:
  echo: false
  warning: false
  eval: true
code-line-numbers: false
colorlinks: true
code-block-bg: darkgray
df-print: default
highlight-style: arrow-dark
biblio-title: References
---
```{r, echo=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, error = FALSE, warning = FALSE, cache = TRUE)
```


\section{Introduction}

This report aims to find the best prediction for past cumulative grocery sales (in dollars) for Dominick's Finer Foods based on an Elastic Net model by @zou2005.

\section{Data}

The data set contains seven years of store-level data collected at Dominick's Finer Foods by the University of Chicago Booth School of Business. The data can be found at \url{https://www.chicagobooth.edu/research/kilts/datasets/dominicks}. The data set contains 50 variables, which stem from:

```{=tex}
\begin{enumerate}
\item customer count files, which contain information about in-store traffic;
\item a demographics file, which contains store-specific demographic data;
\item number identification files, which contain product information.
\end{enumerate}
```
Of the fifty variables, \verb+GROCERY_sum+ is used as dependent variable. Furthermore, four categorical variables are dropped; \verb+STORE+, \verb+CITY+, \verb+ZIP+ and \verb+SPHINDX+. The remaining variables are potential predictor variables.

\section{Method}

To find the optimal set of predictor variables, and there corresponding weights, we use a regression method that penalizes the size of coefficients. The penalty is useful when predictors are collinear, or the number of predictors destabilizes estimation. This data set only consists of 77 observations for 50 variables, hence the number of predictors would destabilize estimation if not penalized.

Let $P(\beta)$ denote a general penalty function. Then, the loss function of the regression equation becomes 
\begin{equation}\label{eq:lossfunction}
L(\beta) = (\mathbf{y}-\mathbf{x}\beta)^{T}(\mathbf{y}-\mathbf{x}\mathbf{\beta}) + \lambda P(\mathbf{\beta}),
\end{equation}
where $\lambda$ is the hyperparameter that determines the strength of the penalty. When $P(\beta) = \beta^{2}$, the regression is called 'ridge' regression. Similarly, when $P(\beta) = |\beta|$, the regression is called 'LASSO' regression. Finally, any convex combination $P(\beta) = \alpha |\beta| + (1-\alpha) \beta^{2}$ of the 'ridge' and 'LASSO' penalty, where $\alpha$ denotes the weight on the 'LASSO' penalty, is called 'elastic net' [compare @zou2005]. We use the elastic net to find the optimal set of predictor variables, since it exploits the LASSO property of variable selection, and the ridge ...

We use an MM-algorithm to estimate the elasting net. The MM-algorithm uses a majorizing function to find the coefficient vector $\mathbf{\beta}$ that minimizes the loss function specified in Equation \ref{eq:lossfunction}. This is because the minimum is not obtained analytically. Let $\epsilon$ denote the desired level of precision. The majorizing function is specified as
\begin{equation*}
L_{MM}(\mathbf{\beta}) = \frac{1}{2}\mathbf{\beta}^{\text{T}}(\frac{1}{n}\mathbf{X}^{\text{T}}\mathbf{X} + \lambda (1-\alpha)\mathbf{I} + \lambda \alpha \mathbf{D})\mathbf{\beta} - \frac{1}{n}\mathbf{\beta}^{\text{T}}\mathbf{X}^{\text{T}}\mathbf{y} + c,
\end{equation*} where $\mathbf{D}$ is a $p \times p$ diagonal matrix with elements
\begin{equation*}
d_{jj} = 1/\max \left(|\beta_{j}^{0}|,\epsilon \right).
\end{equation*}
Furthermore,
\begin{equation*}
c = \frac{1}{2n}\mathbf{y}^{\text{T}}\mathbf{y} + \frac{1}{2}\lambda \alpha \sum_{i=1}^{p}|\beta_{j}^{0}|.
\end{equation*}

To find the optimal penalty strength $\lambda$ and the weight of the elastic net $\alpha$, $K$-fold cross-validation is used. $K$-fold cross-validation starts with splitting the data set into $K$ folds. Subsequently, $K$ iterations are ran as follows: for each iteration $k \in \{1,\ldots,K\}$, fold $k$ is the test set, whereas the remaining $(K-1)$ folds together form the training set. In this report, we use 10 fold cross validations. On the training set, the elastic net is estimated for all 50 values of $\lambda_{i} \in (10^{-2+i})_{i=0}^{12}$. Moreover, for each $\lambda_{i}$, all 50 values of $\alpha$ equally spaced between 0 and 1 are estimated, such that there are $2500$ combinations of the hyperparameters $\alpha$ and $\lambda$ for each fold. All estimated elastic nets (i.e., all combinations of the hyperparameters), are used to predict the dependent variable in the $k$th fold. When this procedure has been completed for all $K$ folds, we have obtained fitted values for every observations in the data set, for all elastic nets. Consequently, prediction errors can be computed.

To evaluate which elastic net performs best (that is, the elastic net that maintains the optimal combination of hyperparameters), the Root Mean Square Error (RMSE) is computed for each elastic net; the elastic net that has the lowest RMSE is deemed to be the best model. Let $\mathbf{\hat{y}}_{\text{test}(k)} = \mathbf{X}_{\text{test}(k)}\mathbf{\hat{\beta}}_{\text{train(k)}}$ denote the fitted values, and $n_{k}$ the number of observations in fold $k$. The RMSE is computed as
\begin{equation*}
\text{RMSE}=\sqrt{\frac{1}{K}\sum_{k=1}^{K}\text{MSE}_{k}},
\end{equation*}
where
\begin{equation*}
\text{MSE} = \frac{1}{n_{k}}(\mathbf{y}_{\text{test}(k)} - \mathbf{\hat{y}}_{\text{test}(k)})^{\text{T}}(\mathbf{y}_{\text{test}(k)} - \mathbf{\hat{y}}_{\text{test}(k)}).
\end{equation*}

To validate the results of our MM model, we compare our coefficient estimates with the ones of the established `glmnet` algorithm. We do not expect the coefficients to match exactly, e.g. due to different optimization algorithms. Hence, we need to measure their differences. We do so with the Absolute Error (AE) 

\begin{equation*}
\text{AE}_i = \left |\hat{\beta}^{GLM}_i - \hat{\beta}^{MM}_i \right |,
\end{equation*}
and the Absolute Percentage Error (APE)
\begin{equation*}
\text{APE}_i = \left |\frac{\hat{\beta}^{GLM}_i - \hat{\beta}^{MM}_i}{\hat{\beta}^{GLM}_i} \right | .
\end{equation*}

where $\hat{\beta}^{GLM}_i$ and $\hat{\beta}^{MM}_i$ are the i-th coefficients of the `glmnet` and MM model, respectively.

Indeed, when applying the `glmnet` and MM implementations to the Dominick's Finer Food data set, we get different coeffciients. Figure 1 shows the mean-absolute-percentage differences (MAPE) of these two implementations for the different predictors. For most predictors, the $APE$ is around to $1 \%$.

```{r mape, fig.cap="Absolute percentage difference between glmnet and MM coefficients."}
plot_coef_rmse + labs(title = "APE Coefficients GLMNET vs. MM")
```

\section{Results}

One explanation for this difference could lie in the algorithms' speed of convergence. For a given data set the MM algorithm might not converge fast enough to provide the same estimates as the `glmnet` implementation. In fact, the cyclical coordinate decent used in `glmnet` is a very efficient algorithm according to @friedman2010.

We investigate this hypothesis by looking at the estimates' differences for lower convergence thresholds $\epsilon$ of the MM algorithm. Figure 2 shows the zero correlation between the Mean Absolute Error and the parameter $\epsilon$: This precision parameter appears to play no role in the discrepancy between the coefficients of `glmnet` and the MM algorithm.

```{r mae, fig.cap="Development of AE fo decreasing precision threshold $\\epsilon$."}
plot_MAPE_eps + labs(title = TeX("Median AE for decreasing $ \\epsilon$"))
```

The uniform difference between the coefficients, and the result that this difference does not depend on the precision parameter $\epsilon$ leads us to believe that there are some structural differences between the `glmnet` and MM algorithm. However, as these discrepancies are low and similar across predictors, both algorithms seem to implement the same model.


\subsection{Test on real data using k-fold}

First we find the optimal Lambda and Alpha value for our MM-algorithm
```{r message = FALSE}
k_fold_alpha_plots(mX,vy,nfolds,vBeta,dEps,dAlpha,lLambda)
```
Then we compare to the optimal lambda and alpha value using the GLMNET package. 


```{r message = FALSE}
plot_cv_GLMET(mX,vy,alpha=dAlpha)

```
\section{Conclusion and Discussion}

# References

\section{Code}
