---
title: "Group_assigment"
author: "Thao Le"
output:
  pdf_document:
    fig_caption: yes
date: "2022-11-02"
header-includes:
  \usepackage{float}
  \usepackage{booktabs} %to thicken table lines
---

```{r include=FALSE}
#' Compute the residuals
#'
#' @param vY vector, y the outcome
#' @param mX matrix, X the predictors
#' @param vBeta vector, the parameters beta
#' @return vResiduals, vector of residuals
#' @examples
#' residuals(vY, mX, vBeta)
residuals <- function(vY, mX, vBeta) {
  vResiduals <- vY - mX %*% vBeta
  return(vResiduals)
}
#' Loss function
#'
#' @param vResiduals vector, the residuals
#' @param vBeta vector, the parameters beta
#' @param dLambda double, the lambda parameter
#' @param dAlpha double, the alpha parameter
#' @return dLoss, double the value of the loss function
#' @examples
#' loss(vResiduals, vBeta, dLambda, dAlpha)
loss <- function(vResiduals, vBeta, dLambda, dAlpha) {
  
  vBeta <- as.matrix(vBeta)
  
  # init
  iN <- nrow(vResiduals)
  dConstr <- dLambda*((1 - dAlpha)/(2*t(vBeta) %*% vBeta) + dAlpha * norm(vBeta, type = "1"))
  
  # compute the loss function
  dLoss <- (2*iN)^(-1) * (t(vResiduals) %*% vResiduals) + dConstr
  
  return(dLoss)
}

#' Change of loss function
#'
#' @param vBeta0 vector, last iterations beta
#' @param vBeta1 vector, this iterations beta
#' @param vResiduals0 vector, residual vector
#' @param vResiduals1, vector, residual vector
#' @param dLambda double, the lambda parameter
#' @param dAlpha double, the alpha parameter
#' @return dLoss, double the value of the loss function
#' @examples
#' loss_change(vBeta0, vBeta1, vResiduals, dLambda, dAlpha)
loss_change <- function(vBeta_0, vBeta_1, vResiduals0, vResiduals1, dLambda, dAlpha) {
  
  # compute the loss
  dLoss0 <- loss(vResiduals0, vBeta_0, dLambda, dAlpha)
  dLoss1 <- loss(vResiduals1, vBeta_1, dLambda, dAlpha)
  
  # compute the change
  dChange <- (dLoss0 - dLoss1)/dLoss0
  
  return(dChange)
}
#' Get the D_jj element, vectorized
#'
#' @param vBeta vector, vector of betas
#' @param dEps double, precision variable
#' @return vD, vector, diagonal elements of D
#' @examples
#' computeDjj(vBeta, dEps)
computeDjj <- Vectorize(function(vBeta, dEps) {
  dD <- 1 / (max(c(abs(vBeta), dEps)))
  return(dD)
}, vectorize.args = "vBeta")
#' Create the D matrix
#'
#' @param vBeta vector, vector of betas
#' @param dEps double, precision variables
#' @return mD, matrix, matrix D with d_jj on diagonal
#' @examples
#' getD(vBeta, dEps)
getD <- function(vBeta, dEps) {
  vDiag <- computeDjj(vBeta, dEps)
  mD <- diag(vDiag)
  return(mD)
}
#' Create the A matrix
#'
#' @param mA1 matrix, pulled out value of A matrix
#' @param mD matrix, the matrix D
#' @param dAlpha double, alpha parameter
#' @param dLambda double, lambda parameter
#' @return mD, matrix, matrix D with d_jj on diagonal
#' @examples
#' getA(mX, mD, dAlpha, dLambda)
getA <- function(mA1, mD, dAlpha, dLambda) {
  mA <- mA1 + dLambda * dAlpha * mD
  return(mA)
  
}
#' Calculate the update for Beta
#'
#' @param mX matrix, the predictor's data
#' @param vY vector, the vector of the outcome variable
#' @param mA matrix, A matrix
#' @return vBetaUpdate, vector, the updated betas
#' @examples
#' updateBeta(mX, vY, mA)
updateBeta <- function(mX, vY, mA) {
  
  # init
  iN <- nrow(mX)
  
  # calculate
  vBetaUpdate <- solve(mA, (1/iN) * (t(mX) %*% vY))
  
  return(vBetaUpdate)
  
}
#' Perform one iteration of the MM
#'
#' @param mX matrix, the predictor's data
#' @param vY vector, the vector of the outcome variable
#' @param vBeta vector, the betas
#' @param dEps double, the precision epsilon
#' @param dAlpha double, the alpha parameter
#' @param dLambda double, the lambda parameter
#' @return vBetaUpdate, vector, the updated betas
#' @examples
#' itElasticNetMM(mX, vY, vBeta, dEps, dAlpha, dLambda)
itElasticNetMM <- function(mX, vY, vBeta, dEps, dAlpha, dLambda) {
  
  # perform one iteration
  mD <- getD(vBeta, dEps)
  mA <- getA(mX, mD, dAlpha, dLambda)
  vBetaUpdate <- updateBeta(mX, vY, mA)
  
  return(vBetaUpdate)
}
#' Run the full MM
#' 
#' @param mX matrix, the predictor's data
#' @param vY vector, the vector of the outcome variable
#' @param vBeta vector, the betas
#' @param dEps double, the precision epsilon
#' @param dAlpha double, the alpha parameter
#' @param dLambda double, the lambda parameter
ElasticNetMM <- function(mX, vY, vBeta, dEps, dAlpha, dLambda) {
  # loop objects
  iP <- ncol(mX)
  iN <- nrow(mX)
  ik <- 1
  dLossChange <- 0
  vBeta0 <- runif(ncol(mX))
  dEps <- 10e-6
  mXtX <- t(mX) %*% mX
  
  # Pull out part one of A
  dA1 <- (1/iN) * mXtX + (dLambda * (1 - dAlpha)) * diag(iP)
  
  while ((ik == 1) | (dLossChange > dEps)) {
    
    # update counter
    ik <- ik + 1
    
    # perform one update
    mD <- getD(vBeta0, dEps)
    mA <- getA(dA1, mD, dAlpha, dLambda)
    vBeta1 <- updateBeta(mX, vY, mA)
    
    # get residuals
    vResiduals0 <- residuals(vY, mX, vBeta0)
    vResiduals1 <- residuals(vY, mX, vBeta1)
    
    # compute loss change
    dLossChange <- loss_change(vBeta0, vBeta1, vResiduals0, vResiduals1, dLambda, dAlpha)
    
    # output
    # print(paste0("Iteration: ", ik, " \n"))
    # print(paste0("Loss Change: ", dLossChange, " \n"))
    # 
    # update the beta
    vBeta0 <- vBeta1
  }
  
  # print("Beta Estimate")
  # print(vBeta0)
  return(vBeta0)
}


```
}

```{r setup, include=FALSE}
if(! require("pacman")) install.packages("pacman")
pacman::p_load(knitr,
               ggplot2,
               png)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:


```{r read data}
current_path <- getwd()
cat("My current working directory is:", current_path)
setwd("/Users/thaole/Desktop/BDS/Block 2/Supervised ML/week 2")
load("supermarket1996.Rdata")
```

Goal: predict grocery_sum through elastic net using the demographic variables as predictors. Omit the variables store, city, ZIP, groccoup_sum and shpindx as predictors
- Write your own function to determine the hyper-parameter through K-fold cross validation

First, we check K-fold using built-in function
```{r results="hide"}
df <- data.frame(supermarket1996)
sub_df <- subset(df, select=-c(STORE, CITY, ZIP, GROCCOUP_sum, SHPINDX))
y <- as.vector(sub_df$GROCERY_sum)      # y variable
X <- as.matrix(sub_df[,-1])
X <- scale(X) # Make columns z-scores of nonfactors

library(glmnet, quietly = TRUE)
result.cv <- cv.glmnet(X, y, alpha = 0, 
                lambda = 10^seq(-2, 10, length.out = 50), nfolds = 10)  
print(result.cv$lambda.min)      # Best cross validated lambda
print(result.cv$lambda.1se)      # Conservative est. of best lambda (1 stdev)

## To plot Root Mean Squared Error (RMSE) to be on the same scale as y:
result.cv$cvm  <- result.cv$cvm^0.5
result.cv$cvup <- result.cv$cvup^0.5
result.cv$cvlo <- result.cv$cvlo^0.5
plot(result.cv, ylab = "Root Mean-Squared Error") 

print(result.cv$lambda.min)      # Best cross validated lambda
# Final run with best cross validated lambda
result <- glmnet(X, y, alpha = 0, lambda = result.cv$lambda.min,
                 intercept = TRUE)  
result$beta


```

Then, we build out own k-fold manually
```{r }

k_fold <- function(mX, vy, nfolds, vBeta, dEps, dAlpha,lLambda){
  #Randomly shuffle the data
  data <-cbind(vy,mX)
  data<-data[sample(nrow(data)),]

  #Create 10 equally size folds
  folds <- cut(seq(1,nrow(data)),breaks=nfolds,labels=FALSE)

  #Perform 10 fold cross validation
  mRSS= matrix(NA, nrow=50, ncol=nfolds) #Create a matrix, each column is the RSS for each test set of the folds
  for(i in 1:ncol(mRSS)){
      #Segment the data by fold 
      testIndexes <- which(folds==i,arr.ind=TRUE)
      testSet <- data[testIndexes, ]
      trainSet <- data[-testIndexes, ]
      #Use the test and train data partitions on Elastic net model
      y_train <- trainSet[,1]
      X_train <- trainSet[,-1]
      y_test <- testSet[,1]
      X_test <- testSet[,-1]
      lRSS = rep(NA, nrow(mRSS))
      for (j in 1:nrow(mRSS)){
        #Calculate RSS for each lambda value
        vBeta_new = ElasticNetMM(X_train, y_train, vBeta, dEps, dAlpha, lLambda[j]) 
        lRSS[j] =sum(residuals(y_test, X_test, vBeta_new)) # is this fine?
      }
      mRSS[,i]=lRSS #Insert RSS values into matrix
  }
  return(mRSS)
}
```
Function to find optimal lambda using CV
```{r }

#init
vy <- as.vector(sub_df$GROCERY_sum)      # y variable
mX <- as.matrix(sub_df[,-1])
mX <- scale(mX) # Make columns z-scores of nonfactors
dEps = 10^(-6)
vBeta= rep(1, ncol(mX))
dAlpha = 0.5
lLambda = 10^seq(-2, 10, length.out = 50)

#Get matrix of RSS over k-fold and 50 lambda
mRSS= k_fold(mX,vy, 10, vBeta, dEps, dAlpha, lLambda)
mRSS

root_mean <- function(mRSS){
  means = sqrt(rowMeans(mRSS))
  return(means)
}
means = mean(mRSS)

#Get index of the min lambda
ind = which.min(means)

lambda_min = lLambda[ind]
cat("The minimum lambda is: ", lambda_min)


plot(means, ylab = "Root Mean-Squared Error")
```
